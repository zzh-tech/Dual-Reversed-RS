<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="Bringing Rolling Shutter Images Alive with Dual Reversed Distortion.">
    <meta name="keywords" content="Rolling shutter correction, Frame interpolation, Dual rolling shutter">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Dual-Reversed-RS</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/zuica_icon.jpg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <link rel="stylesheet" href="./static/css/mycss.css">
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
            <a class="navbar-item" href="https://zzh-tech.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
            </a>

            <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    More Research
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="https://zzh-tech.github.io/Animation-from-Blur/">
                        Animation-from-Blur
                    </a>
                </div>
            </div>
        </div>

    </div>
</nav>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Bringing Rolling Shutter Images Alive with Dual Reversed
                        Distortion</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zzh-tech.github.io/">Zhihang Zhong</a><sup>1,3</sup>,</span>
                        <span class="author-block">
              <a href="https://github.com/ljzycmd">Mingdeng Cao</a><sup>1</sup>,</span>
                        <span class="author-block">
              <a href="https://jimmysuen.github.io/">Xiao Sun</a><sup>2</sup>,</span>
                        <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/wuzhiron/">Zhirong Wu</a><sup>2</sup>,
            </span>
                        <span class="author-block">
              <a href="https://zhongyizhou.net/">Zhongyi Zhou</a><sup>1</sup>,
            </span>
                        <a href="https://scholar.google.com/citations?user=JD-5DKcAAAAJ&hl=zh-CN">Yinqiang
                            Zheng</a><sup>1</sup>,
                        </span>
                        <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/stevelin/">Stephen Lin</a><sup>2</sup>,
            </span>
                        <span class="author-block">
              <a href="https://www.nii.ac.jp/en/faculty/digital_content/sato_imari/">Imari Sato</a><sup>1,3</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>The University of Tokyo,</span>
                        <span class="author-block"><sup>2</sup>Microsoft Research Asia,</span>
                        <span class="author-block"><sup>3</sup>National Institute of Informatics</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- PDF Link. -->
                            <span class="link-block">
                <a href="https://arxiv.org/pdf/2203.06451"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                            <span class="link-block">
                <a href="https://arxiv.org/abs/2203.06451"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!--                            &lt;!&ndash; Video Link. &ndash;&gt;-->
                            <!--                            <span class="link-block">-->
                            <!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
                            <!--                   class="external-link button is-normal is-rounded is-dark">-->
                            <!--                  <span class="icon">-->
                            <!--                      <i class="fab fa-youtube"></i>-->
                            <!--                  </span>-->
                            <!--                  <span>Video</span>-->
                            <!--                </a>-->
                            <!--              </span>-->
                            <!-- Code Link. -->
                            <span class="link-block">
                <a href="https://github.com/zzh-tech/Dual-Reversed-RS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
                            <!--                            &lt;!&ndash; Dataset Link. &ndash;&gt;-->
                            <!--                            <span class="link-block">-->
                            <!--                <a href="https://github.com/google/nerfies/releases/tag/0.1"-->
                            <!--                   class="external-link button is-normal is-rounded is-dark">-->
                            <!--                  <span class="icon">-->
                            <!--                      <i class="far fa-images"></i>-->
                            <!--                  </span>-->
                            <!--                  <span>Data</span>-->
                            <!--                  </a>-->
                            <!--                                </span>-->

                            <div class="is-size-5 publication-authors">
                                <!--                        https://drive.google.com/file/d/1tlg7JHdTvZf6E6WjEg_SokiQf6BQTFRK/view?usp=sharing-->
                                <img src="https://drive.google.com/uc?export=view&id=1ewTyW-SIj45HAsIomQDvKWn5EMgJDBFv"
                                     style="width: 30%;text-align: center">
                                <br>
                                <span style="color:black"><b>Accepted by ECCV'2022 Oral</b></span>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <!--            <video id="teaser" autoplay muted loop playsinline height="100%">-->
            <!--                <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/teaser.mp4"-->
            <!--                        type="video/mp4">-->
            <!--            </video>-->
            <img src="./imgs/teaser.jpg" alt="teaser">
            <h2 class="subtitle has-text-centered">
                <br>
                <b>Dual-Reversed-RS exploits a pair of images captured by dual rolling shutter (RS) cameras with
                    reversed RS directions to extract undistorted global shutter (GS) video clips.</b>
            </h2>
        </div>
    </div>
</section>

<section class="hero is-light is-small">
    <div class="hero-body">
        <div class="container">
            <h2 class="subtitle has-text-centered">Synthetic Demos</h2>
            <div class="carousel results-carousel syn-demo">
                <div class="item1 item-1">
                    <!--                    https://drive.google.com/file/d/1MwPPZGuKXbdxHA_AzrgzVy6kHtXy23em/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1SaM96gHVtWW4umehyoHkrB7ZRIHuJgdZ"
                         alt="demo1" class="item_img">
                </div>
                <div class="item1 item-2">
                    <!--                    https://drive.google.com/file/d/1N7UR_XfZocyvstsxq01HZxfpLHn76Qj_/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1xILQns-MZyE6lgi6wxM0zg63K_Mc9xyf"
                         alt="demo2">
                </div>
                <div class="item1 item-3">
                    <!--                    https://drive.google.com/file/d/1kHjIBdtcViRQMv2MQJI6q1uH3VVISNEk/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1miSF5fVG42jIVymp0rfOMRtyQJgWPT6X"
                         alt="demo3">
                </div>
                <div class="item1 item-4">
                    <!--                    https://drive.google.com/file/d/1U-NlJRfUbL8iDHvvgg4OiqaQC0RReNgY/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1bKlavzH66haoeigxnASv5KoG-SD7OQ-K"
                         alt="demo4">
                </div>
                <div class="item1 item-5">
                    <!--                    https://drive.google.com/file/d/19WR4MYfcb5N-2FJpsj1wefOlouUzjE6w/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1PKwhUvwMCLIOgjb2QLUCCukn8L5Sxadk"
                         alt="demo5">
                </div>
                <div class="item1 item-6">
                    <!--                    https://drive.google.com/file/d/1hyd3pw4DZHH1OHPQ0UF1tAyohmXJsg7l/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1CupO1MZ8X28vbqbyb-j3XWq0EOnlPNTs"
                         alt="demo6">
                </div>
            </div>
        </div>
    </div>
</section>

<br>

<section class="hero is-light is-small">
    <div class="hero-body">
        <div class="container">
            <h2 class="subtitle has-text-centered">Real-world Demos</h2>
            <div class="carousel results-carousel">
                <div class="item2 item-1">
                    <!--                            https://drive.google.com/file/d/1APPuXXE5RtM2-UIftmfkCwrDa83YmJL6/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=19xfFmONWoM77t8DJEnGhFAh5k354GaK3"
                         alt="demo1">
                </div>
                <div class="item2 item-2">
                    <!--                    https://drive.google.com/file/d/1bHdr-lVTWt6RzV6JnjEUKdQQDR-v50bP/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=15bpGvX_R8KLHLqwhImPWnhPFtkk0iqkg"
                         alt="demo2">
                </div>
                <div class="item2 item-3">
                    <!--                    https://drive.google.com/file/d/1-EsHBCCjUWvnLqC3yKTUIFVsoHWCO0gL/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1LR5np02y_bmHEf5Z5ldHoZaVEtoxvPWl"
                         alt="demo3">
                </div>
                <div class="item2 item-4">
                    <!--                    https://drive.google.com/file/d/1vqYgOgOBi6_BSDrJ9JE6o6JviTi2JFTw/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1ZECmnb0-orth4LrhJmkx2BjnvEZu-Ww1"
                         alt="demo4">
                </div>
                <div class="item2 item-5">
                    <!--                    https://drive.google.com/file/d/1-k1841AinNSPNXSDzi92FVWI0UEUzq38/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1x4MCwm_b1ZBuRVp3uvSw6PUWFNGpXPTn"
                         alt="demo5">
                </div>
                <div class="item2 item-6">
                    <!--                    https://drive.google.com/file/d/1facBz0Jfeo2vlquGmVBrKKHeCpH9M-Nu/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1ylF-KoQZyuVspuDN_Z3hfWX2SFfV9imc"
                         alt="demo6">
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Rolling shutter (RS) distortion can be interpreted as the result of picking a row of pixels from
                        instant global shutter (GS) frames over time during the exposure of the RS camera. This means
                        that the information of each instant GS frame is partially, yet sequentially, embedded into the
                        row-dependent distortion. Inspired by this fact, we address the challenging task of reversing
                        this process, <em>i.e.</em>, extracting undistorted GS frames from images suffering from RS
                        distortion. However, since RS distortion is coupled with other factors such as readout settings
                        and the relative velocity of scene elements to the camera, models that only exploit the
                        geometric correlation between temporally adjacent images suffer from poor generality in
                        processing data with different readout settings and dynamic scenes with both camera motion and
                        object motion. In this paper, instead of two consecutive frames, we propose to exploit a pair of
                        images captured by dual RS cameras with reversed RS directions for this highly challenging task.
                        Grounded on the symmetric and complementary nature of dual reversed distortion, we develop a
                        novel end-to-end model, IFED, to generate dual optical flow sequence through iterative learning
                        of the velocity field during the RS time. Extensive experimental results demonstrate that IFED
                        is superior to naive cascade schemes, as well as the state-of-the-art which utilizes adjacent RS
                        images. Most importantly, although it is trained on a synthetic dataset, IFED is shown to be
                        effective at retrieving GS frame sequences from real-world RS distorted images of dynamic
                        scenes.
                    </p>
                    <br>
                </div>
            </div>
        </div>
        <!--/ Abstract. -->

        <!-- Paper video. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Video</h2>
                <div class="publication-video">
                    <!--                    https://drive.google.com/file/d/1TnRYTBv8EbnVJbsePyf2dyRuRqPhJ0it/view?usp=sharing-->
                    <iframe src="https://drive.google.com/uc?export=view&id=1MtEI5IlMLqUp1BJ83smXufdDiw2E7k-G"
                            allow="autoplay; encrypted-media" allowfullscreen></iframe>
                </div>
            </div>
        </div>
        <!--/ Paper video. -->
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">

        <div class="columns is-centered">
            <!-- Visual Effects. &ndash;&gt;-->
            <div class="column">
                <div class="content">
                    <h2 class="title is-3">Rolling Shutter Correction Ambiguity</h2>
                    <p>
                        Correction ambiguity of consecutive frames setup: Suppose there are two similar cylinders, one
                        of them is tilted, as shown in GS view. Then, two RS cameras moving horizontally at the same
                        speed but with different readout time setups can produce the same RS view. Models do not know
                        how much correction is correct facing data beyond their training dataset.
                    </p>
                    <p>
                        Instead of two consecutive frames, we introduce another constraint setting that utilizes
                        intra-frame spatial constraints of dual images taken simultaneously but with reversed
                        distortion. Dual-RS setup can avoid ambiguity because the correct correction pose can be
                        estimated based on the symmetry
                    </p>
                    <br>
                    <img src="./imgs/ambiguity.jpg"
                         style="width: 70%;margin-left: auto;margin-right: auto;display: block">
                    <br>
                    <br>
                </div>
            </div>
        </div>

        <div class="columns is-centered">
            <!-- Visual Effects. &ndash;&gt;-->
            <div class="column">
                <div class="content">
                    <h2 class="title is-3">Methodology</h2>
                    <p>
                        We propose a dual time cube as an RS prior, regress the dual velocity cube to indirectly
                        estimate the dual optical flow for backward warping, and then through an encoder-decoder branch,
                        efficiently merge the symmetric information to extract the potential GS frames.
                    </p>
                    <br>
                    <!--                    https://drive.google.com/file/d/1GYnSx79b6iDVkuZOpBfe1XfKY-Gp8ano/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1dzKXtscy0LvHl-NPtWYuWBJHPcLH2K8X"
                         style="width: 80%;margin-left: auto;margin-right: auto;display: block">
                    <br>
                </div>
            </div>
        </div>

        <div class="columns is-centered">
            <!-- Visual Effects. &ndash;&gt;-->
            <div class="column">
                <div class="content">
                    <h2 class="title is-3">Visual Results</h2>
                    <p>
                        Previous method cannot generalize to either the case of camera-only motion (the left example) or
                        the case of object-only motion (the right example), while ours is robust to different motion
                        patterns.
                    </p>
                    <!--                    https://drive.google.com/file/d/1uG5UwENqTeiqh6gqGT8gIauTf4f2nBFV/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=11dPzn1cS69WIlhra0lYfIHqLoH589jRp">
                    <br>
                    <br>
                    <p>
                        Grounded on the symmetric and complementary nature of dual reversed distortion, our method can
                        successfully generalize to different readout settings without artifacts and undesired
                        distortions.
                    </p>
                    <!--                    https://drive.google.com/file/d/1iRDBnEYHkkiL-DrNTVa18vLsSPcNoFmn/view?usp=sharing-->
                    <img src="https://drive.google.com/uc?export=view&id=1U51Zg7ykyrn2qL05AoZIb3NgtUyeZ08m">
                    <br>
                    <br>
                </div>
            </div>
        </div>

        <!--            &lt;!&ndash; Visual Effects. &ndash;&gt;-->
        <!--            <div class="column">-->
        <!--                <div class="content">-->
        <!--                    <h2 class="title is-3">Visual Effects</h2>-->
        <!--                    <p>-->
        <!--                        Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect-->
        <!--                        would be impossible without nerfies since it would require going through a wall.-->
        <!--                    </p>-->
        <!--                    <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">-->
        <!--                        <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/dollyzoom-stacked.mp4"-->
        <!--                                type="video/mp4">-->
        <!--                    </video>-->
        <!--                </div>-->
        <!--            </div>-->
        <!--            &lt;!&ndash;/ Visual Effects. &ndash;&gt;-->

        <!--            &lt;!&ndash; Matting. &ndash;&gt;-->
        <!--            <div class="column">-->
        <!--                <h2 class="title is-3">Matting</h2>-->
        <!--                <div class="columns is-centered">-->
        <!--                    <div class="column content">-->
        <!--                        <p>-->
        <!--                            As a byproduct of our method, we can also solve the matting problem by ignoring-->
        <!--                            samples that fall outside of a bounding box during rendering.-->
        <!--                        </p>-->
        <!--                        <video id="matting-video" controls playsinline height="100%">-->
        <!--                            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/matting.mp4"-->
        <!--                                    type="video/mp4">-->
        <!--                        </video>-->
        <!--                    </div>-->

        <!--                </div>-->
        <!--            </div>-->
        <!--        </div>-->
        <!--        &lt;!&ndash;/ Matting. &ndash;&gt;-->

        <!--        &lt;!&ndash; Animation. &ndash;&gt;-->
        <!--        <div class="columns is-centered">-->
        <!--            <div class="column is-full-width">-->
        <!--                <h2 class="title is-3">Animation</h2>-->

        <!--                &lt;!&ndash; Interpolating. &ndash;&gt;-->
        <!--                <h3 class="title is-4">Interpolating states</h3>-->
        <!--                <div class="content has-text-justified">-->
        <!--                    <p>-->
        <!--                        We can also animate the scene by interpolating the deformation latent codes of two input-->
        <!--                        frames. Use the slider here to linearly interpolate between the left frame and the right-->
        <!--                        frame.-->
        <!--                    </p>-->
        <!--                </div>-->
        <!--                <div class="columns is-vcentered interpolation-panel">-->
        <!--                    <div class="column is-3 has-text-centered">-->
        <!--                        <img src="https://homes.cs.washington.edu/~kpar/nerfies/images/interpolate_start.jpg"-->
        <!--                             class="interpolation-image"-->
        <!--                             alt="Interpolate start reference image."/>-->
        <!--                        <p>Start Frame</p>-->
        <!--                    </div>-->
        <!--                    <div class="column interpolation-video-column">-->
        <!--                        <div id="interpolation-image-wrapper">-->
        <!--                            Loading...-->
        <!--                        </div>-->
        <!--                        <input class="slider is-fullwidth is-large is-info"-->
        <!--                               id="interpolation-slider"-->
        <!--                               step="1" min="0" max="100" value="0" type="range">-->
        <!--                    </div>-->
        <!--                    <div class="column is-3 has-text-centered">-->
        <!--                        <img src="https://homes.cs.washington.edu/~kpar/nerfies/images/interpolate_end.jpg"-->
        <!--                             class="interpolation-image"-->
        <!--                             alt="Interpolation end reference image."/>-->
        <!--                        <p class="is-bold">End Frame</p>-->
        <!--                    </div>-->
        <!--                </div>-->
        <!--                <br/>-->
        <!--                &lt;!&ndash;/ Interpolating. &ndash;&gt;-->

        <!--                &lt;!&ndash; Re-rendering. &ndash;&gt;-->
        <!--                <h3 class="title is-4">Re-rendering the input video</h3>-->
        <!--                <div class="content has-text-justified">-->
        <!--                    <p>-->
        <!--                        Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel-->
        <!--                        viewpoint such as a stabilized camera by playing back the training deformations.-->
        <!--                    </p>-->
        <!--                </div>-->
        <!--                <div class="content has-text-centered">-->
        <!--                    <video id="replay-video"-->
        <!--                           controls-->
        <!--                           muted-->
        <!--                           preload-->
        <!--                           playsinline-->
        <!--                           width="75%">-->
        <!--                        <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/replay.mp4"-->
        <!--                                type="video/mp4">-->
        <!--                    </video>-->
        <!--                </div>-->
        <!--                &lt;!&ndash;/ Re-rendering. &ndash;&gt;-->

        <!--            </div>-->
        <!--        </div>-->
        <!--        &lt;!&ndash;/ Animation. &ndash;&gt;-->


        <!-- Concurrent Work. -->
        <div class="columns is-centered">
            <div class="column is-full-width">
                <h2 class="title is-3">Related Links</h2>

                <div class="content has-text-justified">
                    <p>
                        We have another interesting work that uses another kind of motion artifact, <em>i.e.</em>,
                        motion blur, to realize image2video:
                        <a href="https://arxiv.org/abs/2207.10123">Animation from Blur: Multi-modal Blur Decomposition
                            with Motion Guidance</a> (<a
                            href="https://zzh-tech.github.io/Animation-from-Blur/">Website</a>,
                        <a href="https://github.com/zzh-tech/Animation-from-Blur">Code</a>).
                    </p>
                    <!--                    <p>-->
                    <!--                        <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a-->
                    <!--                            href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>-->
                    <!--                        both use deformation fields to model non-rigid scenes.-->
                    <!--                    </p>-->
                    <!--                    <p>-->
                    <!--                        Some works model videos with a NeRF by directly modulating the density, such as <a-->
                    <!--                            href="https://video-nerf.github.io/">Video-NeRF</a>, <a-->
                    <!--                            href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a-->
                    <!--                            href="https://neural-3d-video.github.io/">DyNeRF</a>-->
                    <!--                    </p>-->
                    <!--                    <p>-->
                    <!--                        There are probably many more by the time you are reading this. Check out <a-->
                    <!--                            href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>,-->
                    <!--                        and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF-->
                    <!--                        papers</a>.-->
                    <!--                    </p>-->
                </div>
            </div>
        </div>
        <!--/ Concurrent Work. -->

    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{zhong2022bringing,
  title={Bringing rolling shutter images alive with dual reversed distortion},
  author={Zhong, Zhihang and Cao, Mingdeng and Sun, Xiao and Wu, Zhirong and Zhou, Zhongyi and Zheng, Yinqiang and Lin, Stephen and Sato, Imari},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part VII},
  pages={233--249},
  year={2022},
  organization={Springer}
}</code></pre>
    </div>
</section>


<footer class="footer">
    <div class="container">
        <div class="content has-text-centered">
            <a class="icon-link"
               href="https://arxiv.org/pdf/2203.06451">
                <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="https://github.com/zzh-tech" class="external-link" disabled>
                <i class="fab fa-github"></i>
            </a>
        </div>
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        Website template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
